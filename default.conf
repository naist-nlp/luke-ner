--evaluation_strategy epoch
--per_device_train_batch_size 8
--per_device_eval_batch_size 32
--learning_rate 1e-5
--weight_decay 0.01
--optim adamw_hf
--adam_beta1 0.9
--adam_beta2 0.98
--adam_epsilon 1e-6
--max_grad_norm 0.0
--num_train_epochs 5
--lr_scheduler_type linear
--warmup_ratio 0.06
--log_level info
--logging_strategy steps
--logging_steps 100
--save_strategy no
--metric_for_best_model loss
